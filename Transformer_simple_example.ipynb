{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_simple_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AoShuang92/Transformer_simple_example/blob/master/Transformer_simple_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g831xANXh2HY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNemnO18h6PV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72e5ee53-0a8d-484e-e029-4f6931e94148"
      },
      "source": [
        "'''\n",
        "  code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612\n",
        "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
        "              https://github.com/JayParks/transformer\n",
        "'''\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dtype = torch.FloatTensor\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
        "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
        "\n",
        "# Transformer Parameters\n",
        "# Padding Should be Zero\n",
        "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4}\n",
        "src_vocab_size = len(src_vocab)\n",
        "\n",
        "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'S' : 5, 'E' : 6}\n",
        "number_dict = {i: w for i, w in enumerate(tgt_vocab)} #number_dict {0: 'P', 1: 'i', 2: 'want', 3: 'a', 4: 'beer', 5: 'S', 6: 'E'}\n",
        "\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "src_len = 5\n",
        "tgt_len = 5\n",
        "\n",
        "d_model = 512  # Embedding Size\n",
        "d_ff = 2048 # FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_layers = 6  # number of Encoder of Decoder Layer\n",
        "n_heads = 8  # number of heads in Multi-Head Attention\n",
        "\n",
        "def make_batch(sentences):\n",
        "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
        "    #print(\"input_batch\",input_batch) #[[1, 2, 3, 4, 0]]\n",
        "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
        "    #print(\"output_batch\",output_batch) #[[5, 1, 2, 3, 4]]\n",
        "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
        "    #print(\"target_batch\",target_batch) # [[1, 2, 3, 4, 6]]\n",
        "    return Variable(torch.LongTensor(input_batch)), Variable(torch.LongTensor(output_batch)), Variable(torch.LongTensor(target_batch))\n",
        "\n",
        "def get_sinusoid_encoding_table(n_position, d_model):\n",
        "    def cal_angle(position, hid_idx):\n",
        "        #print('hid_idx',hid_idx) #hid_idx is int\n",
        "        #print(\"return1\", position / np.power(10000, 2 * (hid_idx // 2) / d_model))\n",
        "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "    return torch.FloatTensor(sinusoid_table)\n",
        "\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    #print(\"batch_size\",batch_size) #1\n",
        "    #print(\"len_q\",len_q) # 5\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    #print(\"len_k\",len_k) #5\n",
        "    # eq(zero) is PAD token\n",
        "    #only eq(zero) equal=0 is true, others all false\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    #print(\"pad_attn_mask\",pad_attn_mask.size()) #[1, 1, 5]\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
        "\n",
        "def get_attn_subsequent_mask(seq):\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    #print(\"attn_shape\",attn_shape) #[1, 5, 5]\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "    #print(\"subsequent_mask\",subsequent_mask) # to set the upper triangle value as 1, other parts as 0\n",
        "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
        "    #print(\"subsequent_mask\",subsequent_mask.size()) #([1, 5, 5])\n",
        "    return subsequent_mask\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        #print(\"Q\",Q.size()) #([1, 8, 5, 64])\n",
        "        #print(\"K\",K.size()) #([1, 8, 5, 64])\n",
        "        #print(\"V\",V.size()) #([1, 8, 5, 64])\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        #print(\"scores\",scores.size()) #([1, 8, 5, 5])\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        #print(\"scores1\",scores.size()) #([1, 8, 5, 5])\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        #print(\"attn\",attn.size()) #([1, 8, 5, 5])\n",
        "        context = torch.matmul(attn, V)\n",
        "        #print(\"context\",context.size()) #([1, 8, 5, 64])\n",
        "        return context, attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        #print(\"residual\",residual.size()) #([1, 5, 512])\n",
        "        #print(\"batch_size\",batch_size) #1\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        #print(\"q_s\",q_s.size()) #([1, 8, 5, 64])\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        #print(\"k_s\",q_s.size()) #([1, 8, 5, 64])\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "        #print(\"v_s\",q_s.size()) #([1, 8, 5, 64])\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "        #print(\"attn_mask\",attn_mask.size()) #([1, 8, 5, 5])\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        #print(\"context\",context.size()) #[1, 8, 5, 64]\n",
        "        #print(\"attn\",attn.size()) #([1, 8, 5, 5])\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        #print(\"context\",context.size()) #([1, 5, 512])\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        #print(\"output\",output.size())#([1, 5, 512])\n",
        "        #print(\"return_nn\",(nn.LayerNorm(d_model)(output + residual)).size()) #([1, 5, 512])\n",
        "        \n",
        "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n",
        "\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        #kernel_size = 1 : like FC layer\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1) \n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        residual = inputs # inputs : [batch_size, len_q, d_model]\n",
        "        #print(\"residual\",residual.size()) #([1, 5, 512])\n",
        "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
        "        #print(\"output\",output.size()) #([1, 2048, 5])\n",
        "        output = self.conv2(output).transpose(1, 2)\n",
        "        #print(\"output2\",output.size()) #([1, 5, 512])\n",
        "        return nn.LayerNorm(d_model)(output + residual)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        #print(\"enc_outputs\",enc_outputs.size()) #([1, 5, 512])\n",
        "        #print(\"attn\",attn.size()) #([1, 8, 5, 5])\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        #print(\"enc_outputs\",enc_outputs.size()) #([1, 5, 512])\n",
        "        return enc_outputs, attn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.dec_self_attn = MultiHeadAttention()\n",
        "        self.dec_enc_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n",
        "        #print(\"dec_self_attn\",dec_self_attn.size()) #([1, 8, 5, 5])\n",
        "        #print(\"dec_outputs\",dec_outputs.size()) #([1, 5, 512])\n",
        "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
        "        #print(\"dec_outputs1\",dec_outputs.size())#([1, 5, 512])\n",
        "        #print(\"dec_enc_attn\",dec_enc_attn.size())#([1, 8, 5, 5])\n",
        "        dec_outputs = self.pos_ffn(dec_outputs)\n",
        "        #print(\"dec_outputs2\",dec_outputs.size())#([1, 5, 512])\n",
        "        return dec_outputs, dec_self_attn, dec_enc_attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, d_model) #d_model=embedding size=512\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model),freeze=True)\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n",
        "        #print(\"enc_inputs\",enc_inputs) #([[1, 2, 3, 4, 0]])\n",
        "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]])) #same?\n",
        "        #print(\"self.src_emb(enc_inputs)\",self.src_emb(enc_inputs).size()) #([1, 5, 512])\n",
        "        #print(\"self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\",self.pos_emb(torch.LongTensor([[1,2,3,4,0]])).size()) #([1, 5, 512])\n",
        "        #print(\"enc_outputs\",enc_outputs.size()) #[1, 5, 512]\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "        #print(\"enc_self_attn_mask\",enc_self_attn_mask.size()) #[1, 5, 5]\n",
        "        #print(\"enc_self_attn_mask\",enc_self_attn_mask)\n",
        "        enc_self_attns = []\n",
        "        for layer in self.layers:\n",
        "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
        "            enc_self_attns.append(enc_self_attn)\n",
        "        #print(\"enc_self_attns\",len(enc_self_attns)) #6\n",
        "        return enc_outputs, enc_self_attns\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model),freeze=True)\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, dec_inputs, enc_inputs, enc_outputs): # dec_inputs : [batch_size x target_len]\n",
        "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n",
        "        #print(\"dec_outputs\",dec_outputs.size()) #([1, 5, 512])\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)\n",
        "        #print(\"dec_self_attn_pad_mask\",dec_self_attn_pad_mask.size()) #([1, 5, 5])\n",
        "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)\n",
        "        #print(\"dec_self_attn_subsequent_mask\",dec_self_attn_subsequent_mask.size()) #([1, 5, 5])\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
        "        #print(\"dec_self_attn_mask\",dec_self_attn_mask) #torch.gt: return true/false\n",
        "\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
        "        #print(\"dec_enc_attn_mask\",dec_enc_attn_mask.size()) #([1, 5, 5])\n",
        "        dec_self_attns, dec_enc_attns = [], []\n",
        "        for layer in self.layers:\n",
        "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "            dec_self_attns.append(dec_self_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)\n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
        "        #print(\"enc_outputs\",enc_outputs.size()) #[1, 5, 512])\n",
        "        #print(\"enc_self_attns\",len(enc_self_attns)) # 6 #list\n",
        "        #print(\"enc_self_attns\",enc_self_attns[0].size()) #([1, 8, 5, 5])\n",
        "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
        "        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]\n",
        "        #print(\"dec_logits\",dec_logits.size()) #([1, 5, 7])\n",
        "        #print((dec_logits.view(-1, dec_logits.size(-1))).size()) #([5, 7])\n",
        "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns\n",
        "\n",
        "model = Transformer()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def showgraph(attn):\n",
        "    attn = attn[-1].squeeze(0)[0]\n",
        "    attn = attn.squeeze(0).data.numpy()\n",
        "    fig = plt.figure(figsize=(n_heads, n_heads)) # [n_heads, n_heads]\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attn, cmap='viridis')\n",
        "    ax.set_xticklabels(['']+sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n",
        "    ax.set_yticklabels(['']+sentences[2].split(), fontdict={'fontsize': 14})\n",
        "    plt.show()\n",
        "\n",
        "for epoch in range(20):\n",
        "    optimizer.zero_grad()\n",
        "    enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
        "    outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
        "    loss = criterion(outputs, target_batch.contiguous().view(-1))\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Test\n",
        "predict, _, _, _ = model(enc_inputs, dec_inputs)\n",
        "predict = predict.data.max(1, keepdim=True)[1]\n",
        "print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
        "\n",
        "print('first head of last state enc_self_attns')\n",
        "showgraph(enc_self_attns)\n",
        "\n",
        "print('first head of last state dec_self_attns')\n",
        "showgraph(dec_self_attns)\n",
        "\n",
        "print('first head of last state dec_enc_attns')\n",
        "showgraph(dec_enc_attns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 7])\n",
            "Epoch: 0001 cost = 1.927750\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0002 cost = 0.271256\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0003 cost = 0.095381\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0004 cost = 0.012578\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0005 cost = 0.015600\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0006 cost = 0.029517\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0007 cost = 0.010804\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0008 cost = 0.008447\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0009 cost = 0.001204\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0010 cost = 0.000691\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0011 cost = 0.008789\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0012 cost = 0.001940\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0013 cost = 0.000990\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0014 cost = 0.000338\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0015 cost = 0.001743\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0016 cost = 0.000110\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0017 cost = 0.000216\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0018 cost = 0.000091\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0019 cost = 0.000079\n",
            "torch.Size([5, 7])\n",
            "Epoch: 0020 cost = 0.000238\n",
            "torch.Size([5, 7])\n",
            "ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\n",
            "first head of last state enc_self_attns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "first head of last state dec_self_attns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "first head of last state dec_enc_attns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJB-seWdOxDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[[[0. 1. 1. 1. 1.]\n",
        "  [0. 0. 1. 1. 1.]\n",
        "  [0. 0. 0. 1. 1.]\n",
        "  [0. 0. 0. 0. 1.]\n",
        "  [0. 0. 0. 0. 0.]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT8SuGKxmfG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "7ec8ed99-7b67-454c-b76e-bdcbd442e531"
      },
      "source": [
        "k  = np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], 0)\n",
        "k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8f3fea525fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCfQNPmHmhqI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "522b4b87-82fd-416e-bc3b-ac39a0a1793f"
      },
      "source": [
        "pad_attn_mask = torch.Tensor([1,2,3,4,0])\n",
        "pad_attn_mask.expand(1, 5, 5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 2., 3., 4., 0.],\n",
              "         [1., 2., 3., 4., 0.],\n",
              "         [1., 2., 3., 4., 0.],\n",
              "         [1., 2., 3., 4., 0.],\n",
              "         [1., 2., 3., 4., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwVGUKfQzUTr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d817127-328f-4bc7-c802-a1e3f66fb93d"
      },
      "source": [
        "import torch\n",
        "s = torch.Tensor([1,2,3,4,0])\n",
        "s.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtV1Ez9KWjkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = s[:,None,None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCCmznh8WtB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8e78627-a7f1-441b-ed3b-05fa71154890"
      },
      "source": [
        "s.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3bb6XHcWu9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "dbd12c72-9d7f-4dda-ad67-02238942ebde"
      },
      "source": [
        "import numpy as np\n",
        "up_tri = np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1)\n",
        "lower_tri= np.tril([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1)\n",
        "print(\"up_tri\\n\",up_tri)\n",
        "print(\"lower_tri\\n\",lower_tri)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "up_tri\n",
            " [[ 1  2  3]\n",
            " [ 4  5  6]\n",
            " [ 0  8  9]\n",
            " [ 0  0 12]]\n",
            "lower_tri\n",
            " [[ 0  0  0]\n",
            " [ 4  0  0]\n",
            " [ 7  8  0]\n",
            " [10 11 12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2VcPiTLTza7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}